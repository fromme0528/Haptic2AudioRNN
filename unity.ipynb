{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## import libs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "from torch.utils import data as torchData\n",
    "import torch.nn.functional as F\n",
    "import os\n",
    "import csv\n",
    "import pickle\n",
    "import numpy as np\n",
    "import time\n",
    "import util\n",
    "import gc\n",
    "import librosa\n",
    "import soundfile as sf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting Hyperparams "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sequence_len = 101\n",
    "input_size = 10\n",
    "hidden_size= 257\n",
    "num_layers = 2\n",
    "num_classes = 1\n",
    "\n",
    "num_epochs = 30\n",
    "batch_size = 1\n",
    "learning_rate = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "inPath = \"./dataset/accel_250\"\n",
    "inPath2 = \"./dataset/spectrogram\"\n",
    "outPath = \"./output_model/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AudioLoader(torchData.Dataset):\n",
    "\n",
    "    def __init__(self, inPathAccel, inPathAudio,isShuffle=False):\n",
    "\n",
    "        files_accel = os.listdir(inPathAccel) \n",
    "        files_accel = [f for f in files_accel if os.path.splitext(f)[-1] == '.csv']\n",
    "        files_accel.sort()\n",
    "        files_audio = os.listdir(inPathAudio) \n",
    "        files_audio = [f for f in files_audio if os.path.splitext(f)[-1] == '.pickle']\n",
    "        files_audio.sort()\n",
    "\n",
    "        if isShuffle:\n",
    "            random.shuffle(files_accel)\n",
    "\n",
    "        self.inPathAccel = inPathAccel\n",
    "        self.inPathAudio = inPathAudio\n",
    "        self.len = len(files_accel)\n",
    "        self.fileList_accel = files_accel[:self.len]\n",
    "        self.fileList_audio = files_audio[:self.len]\n",
    "        self.isShuffle = isShuffle\n",
    "\n",
    "        print('len of dataset : ',self.len, len(files_audio))\n",
    "\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        with open(os.path.join(self.inPathAccel , self.fileList_accel[idx]),'r',encoding='utf-8') as csvfile:\n",
    "    \n",
    "            rdr = csv.reader(csvfile)\n",
    "            data_accel = [line for line in rdr]\n",
    "\n",
    "            for idx2,each_line in enumerate(data_accel) :\n",
    "\n",
    "                each_line = [float(i) for i in each_line]\n",
    "\n",
    "                #x,y,z 3 axis -> sum(x,y,z) 1 axis and material property\n",
    "                sum_3axis = np.sum(each_line[0:2])\n",
    "                sum_3axis = (sum_3axis-9) / 4\n",
    "                each_line = sum_3axis#[sum_3axis, each_line[-1]]\n",
    "\n",
    "                data_accel[idx2] = each_line\n",
    "\n",
    "            output_data = list()\n",
    "            result = list()\n",
    "\n",
    "            pointer = 0\n",
    "            pointer_bool = True\n",
    "            for i in range(0,sequence_len):\n",
    "\n",
    "                if pointer < 9:#input_size-1\n",
    "                    output_data = data_accel[:pointer+1]\n",
    "                    output_data = np.pad(output_data, (input_size-pointer-1,0),'constant',constant_values=(0))\n",
    "                else:\n",
    "                    output_data = data_accel[pointer-9:pointer+1]\n",
    "\n",
    "                if pointer_bool :\n",
    "                    pointer += 2\n",
    "                else:\n",
    "                    pointer +=3\n",
    "\n",
    "                pointer_bool =  not pointer_bool\n",
    "\n",
    "                if i == sequence_len-1:\n",
    "                    result.append(result[-1])\n",
    "                else:\n",
    "                    result.append(output_data)\n",
    "\n",
    "            result = np.array(result)\n",
    "            result = torch.from_numpy(result)\n",
    "\n",
    "                #output_data = np.array(output_data)\n",
    "                #print(output_data)\n",
    "                #output_data = torch.from_numpy(output_data)\n",
    "\n",
    "            #with open(os.path.join(self.inPathAudio, self.fileList_audio[idx]),'rb') as fs:\n",
    "\n",
    "            with open(os.path.join(self.inPathAudio+'/'+self.fileList_audio[idx]),'rb') as fs:\n",
    "                spectro = pickle.load(fs)\n",
    "                #print(self.inPathAudio+'/'+self.fileList_audio[idx])\n",
    "                spectro = np.array(spectro)\n",
    "                spectro = np.log1p(spectro)\n",
    "                spectro /= 2.2\n",
    "                label = torch.from_numpy(spectro)\n",
    "            return result, label #data_audio #input-label\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.len\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len of dataset :  201 201\n"
     ]
    }
   ],
   "source": [
    "dataSet = AudioLoader(os.path.join(inPath),os.path.join(inPath2))\n",
    "\n",
    "trainLoader = torchData.DataLoader(\n",
    "    dataset = dataSet,\n",
    "    batch_size = batch_size,\n",
    "    shuffle = False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Haptic2AudioRNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Haptic2AudioRNN,self).__init__()\n",
    "        self.model = nn.RNN(input_size =input_size, hidden_size = hidden_size,\n",
    "                             num_layers = num_layers, batch_first = True, nonlinearity = 'relu')\n",
    "\n",
    "    def forward(self, x, hidden):       \n",
    "        \n",
    "        x = x.view(batch_size, sequence_len, input_size)\n",
    "        out, hidden = self.model(x, hidden)\n",
    "        return hidden, out.view(-1, num_classes)\n",
    "\n",
    "    def init_hidden(self):\n",
    "        # Set initial states \n",
    "        return Variable(torch.zeros(num_layers,batch_size, hidden_size).type(torch.DoubleTensor))\n",
    "\n",
    "    \n",
    "def save(outPath, prefix = ''):\n",
    "\n",
    "    timeText = util.getTime()\n",
    "\n",
    "    if not prefix == '':\n",
    "\n",
    "        prefix = prefix + '_'\n",
    "\n",
    "    try:\n",
    "        torch.save(self.model.cpu(), os.path.join(outPath, timeText + prefix+'rnn.model'))\n",
    "    except:\n",
    "        print('error : can\\'t save model')\n",
    "\n",
    "    else:\n",
    "        print('successfully saved model - ', timeText + prefix)\n",
    "        \n",
    "        \n",
    "def load(inPath, time = '', num = 1):\n",
    "\n",
    "    try:\n",
    "        #load the model files which are created lastly\n",
    "        files = os.listdir(inPath)\n",
    "        files = [f for f in files if os.path.splitext(f)[-1] == '.model']\n",
    "        #files.sort(reverse = True)\n",
    "        files.sort()\n",
    "        #timeText = files[0][:10] + '_'\n",
    "        #self.model = torch.load(os.path.join(inPath, timeText + prefix + 'rnn.model'))\n",
    "        model = torch.load(os.path.join(inPath, files[-num]))\n",
    "        #if torch.cuda.is_available():\n",
    "        #    self.model.cuda()\n",
    "\n",
    "    except:\n",
    "        print('error : can\\'t load model')\n",
    "\n",
    "    else:\n",
    "        #print('successfully loaded all model - ',timeText + prefix)\n",
    "        print('successfully loaded all model - ',files[-num])\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = Haptic2AudioRNN()\n",
    "model.double()\n",
    "#def criterion(y, outputs):\n",
    "#    y = \n",
    "criterion1 = nn.KLDivLoss(size_average=True)#size_average = True\n",
    "optimizer = torch.optim.Adam(model.parameters(),lr = learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print('Train Start...')\n",
    "\n",
    "lossHistory = list()\n",
    "\n",
    "hidden= None\n",
    "hidden = model.init_hidden()\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "\n",
    "    start_time = time.time()\n",
    "    \n",
    "    for idx, data in enumerate(trainLoader):#,0\n",
    "\n",
    "        x, y = data\n",
    "        x = Variable(x)\n",
    "\n",
    "        y = Variable(y.type(torch.DoubleTensor), requires_grad = False)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss = 0\n",
    "        \n",
    "        hidden, outputs = model.forward(x,hidden)\n",
    "        #print(hidden)\n",
    "        #print(outputs)\n",
    "        #print(\"- outputs\")\n",
    "        print(torch.mean(hidden))\n",
    "        outputs = outputs.view(101,257)\n",
    "\n",
    "        '''\n",
    "        print(\"- y\")\n",
    "        util.printInfo(y)\n",
    "        util.printInfo(F.softmax(y.view(101*257)))\n",
    "        util.printInfo(F.log_softmax(F.softmax(y.view(101*257))))\n",
    "        print(torch.sum(F.softmax(y.view(101*257))))\n",
    "        \n",
    "        print(\"- outputs\")\n",
    "        util.printInfo(outputs)\n",
    "        util.printInfo(F.softmax(outputs.view(101*257)))\n",
    "        util.printInfo(F.softmax(F.softmax(outputs.view(101*257))))\n",
    "        '''\n",
    "        \n",
    "        loss = (torch.sum(torch.pow((y -  outputs),2)) / (101*257)) #+ criterion(F.log_softmax(F.softmax(outputs.view(101,257))), F.softmax(F.softmax(y.view(101,257))))\n",
    "        \n",
    "        #loss = criterion(F.log_softmax(outputs/torch.sum(outputs),dim=1),F.softmax(y/torch.sum(y),dim=1))\n",
    "        #loss = torch.sum(torch.pow((y -  outputs),2)) / 8000.0\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        print(loss.data[0])\n",
    "        \n",
    "        if idx%20 == 0:\n",
    "            #util.printInfo(y)\n",
    "            #util.printInfo(outputs)\n",
    "            print ('Epoch [%d/%d], Iter [%d/%d], Loss: %.8f'\n",
    "               %(epoch+1, num_epochs,idx+1, 201, loss.data[0]))\n",
    "            print(\"--- %s seconds for epoch ---\" % (time.time() - start_time))\n",
    "            \n",
    "        lossHistory.append((epoch,idx,loss.data[0]))\n",
    "\n",
    "    save(outPath, 'epoch' + str(epoch))\n",
    "\n",
    "save(outPath, 'final')\n",
    "print(lossHistory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
