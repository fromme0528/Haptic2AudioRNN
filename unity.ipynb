{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## import libs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "from torch.utils import data as torchData\n",
    "import torch.nn.functional as F\n",
    "import os\n",
    "import csv\n",
    "import pickle\n",
    "import numpy as np\n",
    "import time\n",
    "import util\n",
    "import gc\n",
    "import librosa\n",
    "import soundfile as sf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting Hyperparams "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sequence_len = 101\n",
    "input_size = 10\n",
    "hidden_size= 257\n",
    "num_layers = 2\n",
    "num_classes = 1\n",
    "\n",
    "num_epochs = 30\n",
    "batch_size = 1\n",
    "learning_rate = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "inPath = \"./dataset/accel_250\"\n",
    "inPath2 = \"./dataset/spectrogram\"\n",
    "outPath = \"./output_model/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class AudioLoader(torchData.Dataset):\n",
    "\n",
    "    def __init__(self, inPathAccel, inPathAudio,isShuffle=False):\n",
    "\n",
    "        files_accel = os.listdir(inPathAccel) \n",
    "        files_accel = [f for f in files_accel if os.path.splitext(f)[-1] == '.csv']\n",
    "        files_accel.sort()\n",
    "        files_audio = os.listdir(inPathAudio) \n",
    "        files_audio = [f for f in files_audio if os.path.splitext(f)[-1] == '.pickle']\n",
    "        files_audio.sort()\n",
    "\n",
    "        if isShuffle:\n",
    "            random.shuffle(files_accel)\n",
    "\n",
    "        self.inPathAccel = inPathAccel\n",
    "        self.inPathAudio = inPathAudio\n",
    "        self.len = len(files_accel)\n",
    "        self.fileList_accel = files_accel[:self.len]\n",
    "        self.fileList_audio = files_audio[:self.len]\n",
    "        self.isShuffle = isShuffle\n",
    "\n",
    "        print('len of dataset : ',self.len, len(files_audio))\n",
    "\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        with open(os.path.join(\"./dataset/accel_250/\",\"accel_steel_0.csv\"),'r',encoding='utf-8') as csvfile:\n",
    "    \n",
    "            rdr = csv.reader(csvfile)\n",
    "            data_accel = [line for line in rdr]\n",
    "\n",
    "            for idx2,each_line in enumerate(data_accel) :\n",
    "\n",
    "                each_line = [float(i) for i in each_line]\n",
    "\n",
    "                #x,y,z 3 axis -> sum(x,y,z) 1 axis and material property\n",
    "                sum_3axis = np.sum(each_line[0:2])\n",
    "                sum_3axis /= 10\n",
    "                each_line = sum_3axis#[sum_3axis, each_line[-1]]\n",
    "\n",
    "                data_accel[idx2] = each_line\n",
    "\n",
    "            output_data = list()\n",
    "            result = list()\n",
    "\n",
    "            pointer = 0\n",
    "            pointer_bool = True\n",
    "            for i in range(0,sequence_len):\n",
    "\n",
    "                if pointer < 9:#input_size-1\n",
    "                    output_data = data_accel[:pointer+1]\n",
    "                    output_data = np.pad(output_data, (input_size-pointer-1,0),'constant',constant_values=(0))\n",
    "                else:\n",
    "                    output_data = data_accel[pointer-9:pointer+1]\n",
    "\n",
    "                if pointer_bool :\n",
    "                    pointer += 2\n",
    "                else:\n",
    "                    pointer +=3\n",
    "\n",
    "                pointer_bool =  not pointer_bool\n",
    "\n",
    "                if i == sequence_len-1:\n",
    "                    result.append(result[-1])\n",
    "                else:\n",
    "                    result.append(output_data)\n",
    "\n",
    "            result = np.array(result)\n",
    "            result = torch.from_numpy(result)\n",
    "\n",
    "                #output_data = np.array(output_data)\n",
    "                #print(output_data)\n",
    "                #output_data = torch.from_numpy(output_data)\n",
    "\n",
    "            #with open(os.path.join(self.inPathAudio, self.fileList_audio[idx]),'rb') as fs:\n",
    "\n",
    "            with open(os.path.join(self.inPathAudio+'/'+self.fileList_audio[idx]),'rb') as fs:\n",
    "                spectro = pickle.load(fs)\n",
    "                #print(self.inPathAudio+'/'+self.fileList_audio[idx])\n",
    "                spectro = np.array(spectro)\n",
    "                label = torch.from_numpy(spectro)\n",
    "            return result, label #data_audio #input-label\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.len\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len of dataset :  201 201\n"
     ]
    }
   ],
   "source": [
    "dataSet = AudioLoader(os.path.join(inPath),os.path.join(inPath2))\n",
    "\n",
    "trainLoader = torchData.DataLoader(\n",
    "    dataset = dataSet,\n",
    "    batch_size = batch_size,\n",
    "    shuffle = False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Haptic2AudioRNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Haptic2AudioRNN,self).__init__()\n",
    "        self.model = nn.RNN(input_size =input_size, hidden_size = hidden_size,\n",
    "                             num_layers = num_layers, batch_first = True, nonlinearity='relu')\n",
    "\n",
    "    def forward(self, x, hidden):       \n",
    "        \n",
    "        x = x.view(batch_size, sequence_len, input_size)\n",
    "        out, hidden = self.model(x, hidden)\n",
    "        return hidden, out.view(-1, num_classes)\n",
    "\n",
    "    def init_hidden(self):\n",
    "        # Set initial states \n",
    "        return Variable(torch.zeros(num_layers,batch_size, hidden_size).type(torch.DoubleTensor))\n",
    "\n",
    "    \n",
    "def save(outPath, prefix = ''):\n",
    "\n",
    "    timeText = util.getTime()\n",
    "\n",
    "    if not prefix == '':\n",
    "\n",
    "        prefix = prefix + '_'\n",
    "\n",
    "    try:\n",
    "        torch.save(self.model.cpu(), os.path.join(outPath, timeText + prefix+'rnn.model'))\n",
    "    except:\n",
    "        print('error : can\\'t save model')\n",
    "\n",
    "    else:\n",
    "        print('successfully saved model - ', timeText + prefix)\n",
    "        \n",
    "        \n",
    "def load(inPath, time = '', num = 1):\n",
    "\n",
    "    try:\n",
    "        #load the model files which are created lastly\n",
    "        files = os.listdir(inPath)\n",
    "        files = [f for f in files if os.path.splitext(f)[-1] == '.model']\n",
    "        #files.sort(reverse = True)\n",
    "        files.sort()\n",
    "        #timeText = files[0][:10] + '_'\n",
    "        #self.model = torch.load(os.path.join(inPath, timeText + prefix + 'rnn.model'))\n",
    "        model = torch.load(os.path.join(inPath, files[-num]))\n",
    "        #if torch.cuda.is_available():\n",
    "        #    self.model.cuda()\n",
    "\n",
    "    except:\n",
    "        print('error : can\\'t load model')\n",
    "\n",
    "    else:\n",
    "        #print('successfully loaded all model - ',timeText + prefix)\n",
    "        print('successfully loaded all model - ',files[-num])\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Start...\n",
      "mean:  0.03204322499452612 ,var: 0.004659598361726536 ,max: 1.7660216093063354 ,min: 1.347905708826147e-06\n",
      "mean:  0.023207105604946036 ,var: 0.0012807204866200701 ,max: 0.23156123345705637 ,min: 0.0\n",
      "Epoch [1/30], Iter [1/201], Loss: 0.40152287\n",
      "--- 0.10505986213684082 seconds for epoch ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dxplaboratory/anaconda3/lib/python3.5/site-packages/ipykernel/__main__.py:59: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "/home/dxplaboratory/anaconda3/lib/python3.5/site-packages/ipykernel/__main__.py:59: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean:  0.03191189173167216 ,var: 0.0028273903443647836 ,max: 1.213320016860962 ,min: 4.336019898687482e-08\n",
      "mean:  0.007686976516437513 ,var: 0.00057053213674933 ,max: 0.18203328442597144 ,min: 0.0\n",
      "Epoch [1/30], Iter [21/201], Loss: 0.12449338\n",
      "--- 20.12772536277771 seconds for epoch ---\n",
      "mean:  0.04467597218555891 ,var: 0.03454167200119942 ,max: 7.260131359100342 ,min: 1.035178942565551e-09\n",
      "mean:  0.007585357469711439 ,var: 0.0005686270773072991 ,max: 0.1346017375124735 ,min: 0.0\n",
      "Epoch [1/30], Iter [41/201], Loss: 7.16556321\n",
      "--- 68.87751841545105 seconds for epoch ---\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print('Train Start...')\n",
    "\n",
    "for i in range(1,4):\n",
    "    \n",
    "    model = Haptic2AudioRNN()\n",
    "    model.double()\n",
    "    learning_rate = 0.01 * (0.1**i)\n",
    "    criterion = nn.KLDivLoss(size_average=False)#size_average = True\n",
    "    optimizer = torch.optim.Adam(model.parameters(),lr = learning_rate)\n",
    "    \n",
    "    \n",
    "    lossHistory = list()\n",
    "\n",
    "    hidden= None\n",
    "    hidden = model.init_hidden()\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "\n",
    "        start_time = time.time()\n",
    "\n",
    "        for idx, data in enumerate(trainLoader):#,0\n",
    "\n",
    "            x, y = data\n",
    "            x = Variable(x)\n",
    "\n",
    "            y = Variable(y.type(torch.DoubleTensor), requires_grad = False)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss = 0\n",
    "\n",
    "            hidden, outputs = model.forward(x,hidden)\n",
    "            #print(hidden)\n",
    "            #print(outputs)\n",
    "            #print(\"- outputs\")\n",
    "            #print(torch.mean(hidden))\n",
    "            outputs = outputs.view(101,257)\n",
    "\n",
    "            #print(\"- y\")\n",
    "            #util.printInfo(y)\n",
    "            #util.printInfo(F.softmax(y.view(101,257)))\n",
    "\n",
    "            #print(torch.sum(F.softmax(y.view(101,257))[0][0]))\n",
    "    #        util.printInfo(F.log_softmax(F.softmax(y.view(101*257))))\n",
    "            #print(torch.sum(F.softmax(y.view(101,257))))\n",
    "\n",
    "            #print(\"- outputs\")\n",
    "            #util.printInfo(outputs)\n",
    "            #util.printInfo(F.softmax(outputs.view(101*257)))\n",
    "            #util.printInfo(F.softmax(F.softmax(outputs.view(101*257))))\n",
    "\n",
    "            #print(criterion(F.log_softmax(outputs.view(101,257)), F.softmax(y.view(101,257))))        \n",
    "            #print((torch.mean((y -  outputs)**2)) )\n",
    "            #print(criterion(F.log_softmax(F.softmax(outputs.view(101,257))), F.softmax(F.softmax(y.view(101,257)))))\n",
    "\n",
    "            #print(criterion(F.log_softmax(outputs/torch.sum(outputs),dim=1),F.softmax(y/torch.sum(y),dim=1)))\n",
    "\n",
    "\n",
    "            loss = (torch.mean((y -  outputs)**2)) + criterion(F.log_softmax(outputs.view(101,257)), F.softmax(y.view(101,257)))\n",
    "            #+ criterion(F.log_softmax(F.softmax(outputs.view(101,257))), F.softmax(F.softmax(y.view(101,257))))\n",
    "    #        #        print(torch.sum(torch.pow((y -  outputs),2)) / (101*257))\n",
    "            #loss = torch.sum(torch.pow((y -  outputs),2)) / 8000.0\n",
    "            \n",
    "            loss.backward(retain_graph=True)\n",
    "                    #retain_graph=True\n",
    "            optimizer.step()\n",
    "\n",
    "            #print(loss.data[0])\n",
    "\n",
    "            if idx%20 == 0:\n",
    "                util.printInfo(y)\n",
    "                util.printInfo(outputs)\n",
    "                print ('Epoch [%d/%d], Iter [%d/%d], Loss: %.8f'\n",
    "                   %(epoch+1, num_epochs,idx+1, 201, loss.data[0]))\n",
    "                print(\"--- %s seconds for epoch ---\" % (time.time() - start_time))\n",
    "\n",
    "            lossHistory.append((epoch,idx,loss.data[0]))\n",
    "            gc.collect()\n",
    "\n",
    "\n",
    "        save(outPath, 'epoch' + str(epoch))\n",
    "\n",
    "    save(outPath, 'final')\n",
    "    print(lossHistory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "print('Train Start...')\n",
    "\n",
    "for i in range(1,4):\n",
    "    \n",
    "    model = Haptic2AudioRNN()\n",
    "    model.double()\n",
    "    learning_rate = 0.01 * (0.1**i)\n",
    "    criterion = nn.KLDivLoss(size_average=False)#size_average = True\n",
    "    optimizer = torch.optim.Adam(model.parameters(),lr = learning_rate)\n",
    "    \n",
    "    lossHistory = list()\n",
    "\n",
    "    hidden= None\n",
    "    hidden = model.init_hidden()\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "\n",
    "        start_time = time.time()\n",
    "\n",
    "        for idx, data in enumerate(trainLoader):#,0\n",
    "\n",
    "            x, y = data\n",
    "            x = Variable(x)\n",
    "\n",
    "            y = Variable(y.type(torch.DoubleTensor), requires_grad = False)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss = 0\n",
    "\n",
    "            hidden, outputs = model.forward(x,hidden)\n",
    "            #print(hidden)\n",
    "            #print(outputs)\n",
    "            #print(\"- outputs\")\n",
    "            #print(torch.mean(hidden))\n",
    "            outputs = outputs.view(101,257)\n",
    "\n",
    "            #print(\"- y\")\n",
    "            #util.printInfo(y)\n",
    "            #util.printInfo(F.softmax(y.view(101,257)))\n",
    "\n",
    "            #print(torch.sum(F.softmax(y.view(101,257))[0][0]))\n",
    "    #        util.printInfo(F.log_softmax(F.softmax(y.view(101*257))))\n",
    "            #print(torch.sum(F.softmax(y.view(101,257))))\n",
    "\n",
    "            #print(\"- outputs\")\n",
    "            #util.printInfo(outputs)\n",
    "            #util.printInfo(F.softmax(outputs.view(101*257)))\n",
    "            #util.printInfo(F.softmax(F.softmax(outputs.view(101*257))))\n",
    "\n",
    "            #print(criterion(F.log_softmax(outputs.view(101,257)), F.softmax(y.view(101,257))))        \n",
    "            #print((torch.mean((y -  outputs)**2)) )\n",
    "            #print(criterion(F.log_softmax(F.softmax(outputs.view(101,257))), F.softmax(F.softmax(y.view(101,257)))))\n",
    "\n",
    "            #print(criterion(F.log_softmax(outputs/torch.sum(outputs),dim=1),F.softmax(y/torch.sum(y),dim=1)))\n",
    "\n",
    "\n",
    "            loss = (torch.mean((y -  outputs)**2)) + criterion(F.log_softmax(outputs.view(101,257)), F.softmax(y.view(101,257)))\n",
    "            #+ criterion(F.log_softmax(F.softmax(outputs.view(101,257))), F.softmax(F.softmax(y.view(101,257))))\n",
    "    #        #        print(torch.sum(torch.pow((y -  outputs),2)) / (101*257))\n",
    "            #loss = torch.sum(torch.pow((y -  outputs),2)) / 8000.0\n",
    "\n",
    "            loss.backward(retain_graph=True)\n",
    "            optimizer.step()\n",
    "\n",
    "            #print(loss.data[0])\n",
    "\n",
    "            if idx%20 == 0:\n",
    "                util.printInfo(y)\n",
    "                util.printInfo(outputs)\n",
    "                print ('Epoch [%d/%d], Iter [%d/%d], Loss: %.8f'\n",
    "                   %(epoch+1, num_epochs,idx+1, 201, loss.data[0]))\n",
    "                print(\"--- %s seconds for epoch ---\" % (time.time() - start_time))\n",
    "\n",
    "            lossHistory.append((epoch,idx,loss.data[0]))\n",
    "\n",
    "        save(outPath, 'epoch' + str(epoch))\n",
    "\n",
    "    save(outPath, 'final')\n",
    "    print(lossHistory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
